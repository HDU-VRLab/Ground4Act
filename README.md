# Ground4Act: Leveraging Visual-Language Model for Collaborative Pushing and Grasping in Clutter
This repository contains the implementation of Ground4Act, a two-stage approach for collaborative pushing and grasping in clutter using a visual-language model.
[**Demonstration**](#demonstration) | [**Installation**](#installation) | [**Model Weights**](#model-weights) | [**Code Structure**](#code-structure) | [**Related Work**](#related-work) | [**BibTeX**](#bibtex)
